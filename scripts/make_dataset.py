"""
Script to download and prepare MovieLens dataset
"""

import os
import pathlib
import requests
import zipfile
import pandas as pd
import json
from tqdm import tqdm

# Define paths
CURRENT_DIR = pathlib.Path(__file__).parent.resolve()
DATA_DIR = CURRENT_DIR.parent / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"

# Create directories if they don't exist
RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)

# URLs
MOVIELENS_100K_URL = "https://files.grouplens.org/datasets/movielens/ml-100k.zip"


def download_dataset():
    """
    Download MovieLens 100K dataset if it doesn't exist
    """
    target_dir = RAW_DATA_DIR / "ml-100k"

    if target_dir.exists() and any(target_dir.iterdir()):
        print("MovieLens 100K dataset already exists.")
        return

    print("Downloading MovieLens 100K dataset...")

    # Download the dataset
    response = requests.get(MOVIELENS_100K_URL, stream=True)
    response.raise_for_status()

    # Save the zip file
    zip_path = RAW_DATA_DIR / "ml-100k.zip"
    total_size = int(response.headers.get("content-length", 0))

    with open(zip_path, "wb") as f, tqdm(
        total=total_size, unit="B", unit_scale=True, unit_divisor=1024
    ) as pbar:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                pbar.update(len(chunk))

    # Extract the zip file
    print("Extracting dataset...")
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(RAW_DATA_DIR)

    # Clean up zip file
    zip_path.unlink()

    print("Dataset downloaded and extracted successfully.")


def create_id_mappings():
    """
    Create ID mappings for users and items
    """
    print("Creating ID mappings...")

    # Load ratings data
    ratings_file = RAW_DATA_DIR / "ml-100k" / "u.data"
    ratings = pd.read_csv(
        ratings_file, sep="\t", names=["user_id", "item_id", "rating", "timestamp"]
    )

    # Create ID mappings
    user_ids = ratings["user_id"].unique()
    item_ids = ratings["item_id"].unique()

    user_id_map = {str(uid): idx for idx, uid in enumerate(user_ids)}
    item_id_map = {str(iid): idx for idx, iid in enumerate(item_ids)}

    # Save ID mappings
    with open(PROCESSED_DATA_DIR / "user_id_map.json", "w") as f:
        json.dump(user_id_map, f)

    with open(PROCESSED_DATA_DIR / "item_id_map.json", "w") as f:
        json.dump(item_id_map, f)

    print(f"Created mappings for {len(user_ids)} users and {len(item_ids)} items.")


def prepare_dummy_embeddings():
    """
    Prepare placeholder embeddings files for user and movie profiles
    These will be replaced by actual embeddings generated by the LLM
    """
    print("Creating placeholder embedding files...")

    # Load ID mappings
    with open(PROCESSED_DATA_DIR / "user_id_map.json", "r") as f:
        user_id_map = json.load(f)

    with open(PROCESSED_DATA_DIR / "item_id_map.json", "r") as f:
        item_id_map = json.load(f)

    # Create placeholder embeddings with empty lists
    embedding_dim = 768  # Standard embedding dimension

    # User embeddings
    user_embeddings = []
    for user_id in user_id_map.keys():
        user_embeddings.append(
            {
                "user_id": user_id,
                "embedding": [0.0] * embedding_dim,
                "llm_text": f"User {user_id} profile",
            }
        )

    # Movie embeddings
    movie_embeddings = []
    for item_id in item_id_map.keys():
        movie_embeddings.append(
            {
                "item_id": item_id,
                "embedding": [0.0] * embedding_dim,
                "llm_text": f"Movie {item_id} description",
            }
        )

    # Save as JSON
    with open(PROCESSED_DATA_DIR / "user_embeddings.json", "w") as f:
        json.dump(user_embeddings, f)

    with open(PROCESSED_DATA_DIR / "movie_embeddings.json", "w") as f:
        json.dump(movie_embeddings, f)

    print("Created placeholder embedding files.")
    print(
        "Note: These are placeholder files. Run the profile embedding generation scripts to create actual embeddings."
    )


def main():
    """
    Main function to prepare the dataset
    """
    print("Preparing MovieLens 100K dataset...")

    # Download the dataset
    download_dataset()

    # Create ID mappings
    create_id_mappings()

    # Prepare embeddings
    prepare_dummy_embeddings()

    print("Dataset preparation completed.")
    print("\nNext steps:")
    print("1. Generate text profiles: python scripts/build_features.py")
    print("2. Train the model: python models/dnn/model_v2.py")


if __name__ == "__main__":
    main()
